---
title: æœºå™¨å­¦ä¹ æ–¹æ³•(ä¸€)ï¼šç‰©ä»¥ç±»èš äººä»¥ç¾¤åˆ† Kè¿‘é‚»ç®—æ³•
date: 2019-07-18 13:29:48
tags:
- Machine Learning
- ç¬”è®°
toc: true
reward: true
---

## ä¸€ã€KNNç®—æ³•æ¦‚è¿°

kè¿‘é‚»ç®—æ³•æ˜¯ä¸€ç§åŸºæœ¬**åˆ†ç±»**ä¸**å›å½’**çš„æ–¹æ³•ï¼Œåœ¨ç©ºé—´ä¸­å¦‚æœä¸€ä¸ªæ ·æœ¬é™„è¿‘çš„kä¸ªæœ€è¿‘æ ·æœ¬çš„å¤§å¤šæ•°å±äºæŸä¸€ä¸ªç±»åˆ«ï¼Œåˆ™è¯¥æ ·æœ¬ä¹Ÿå±äºè¿™ä¸ªç±»åˆ«ã€‚å³ç»™å®šä¸€ä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œå¯¹æ–°çš„è¾“å…¥å®ä¾‹è¿›è¡Œå½’ç±»ï¼Œç±»åˆ«å³ä¸ºä¸è¯¥å®ä¾‹æœ€è¿‘çš„kä¸ªå®ä¾‹ä¸­çš„å¤šæ•°å±äºçš„ç±»ã€‚k=1æ—¶ï¼Œä¸ºæœ€è¿‘é‚»ç®—æ³•ã€‚

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæœ‰ä¸¤ç§ä¸åŒçš„æ ·æœ¬æ•°æ®ï¼Œç»¿è‰²çš„åœ†ç‚¹ä»£è¡¨çš„å³ä¸ºå¾…åˆ†ç±»çš„æ•°æ®ã€‚é’ˆå¯¹ä¸åŒçš„kå€¼ï¼Œåˆ†ç±»ç»“æœä¹Ÿä¼šæœ‰æ‰€ä¸åŒã€‚

<div align=center>
<img src="/KNN.png" alt="KNNä¾‹å­" />
</div>

k=3æ—¶ï¼Œç»¿è‰²åœ†ç‚¹çš„æœ€è¿‘çš„3ä¸ªé‚»å±…æ˜¯2ä¸ªçº¢è‰²å°ä¸‰è§’å½¢å’Œ1ä¸ªè“è‰²å°æ­£æ–¹å½¢ï¼Œå°‘æ•°ä»å±äºå¤šæ•°ï¼ŒåŸºäºç»Ÿè®¡çš„æ–¹æ³•ï¼Œåˆ¤å®šç»¿è‰²çš„è¿™ä¸ªå¾…åˆ†ç±»ç‚¹å±äºçº¢è‰²çš„ä¸‰è§’å½¢ä¸€ç±»ã€‚

k=5æ—¶ï¼Œç»¿è‰²åœ†ç‚¹çš„æœ€è¿‘çš„5ä¸ªé‚»å±…æ˜¯2ä¸ªçº¢è‰²ä¸‰è§’å½¢å’Œ3ä¸ªè“è‰²çš„æ­£æ–¹å½¢ï¼Œè¿˜æ˜¯å°‘æ•°ä»å±äºå¤šæ•°ï¼ŒåŸºäºç»Ÿè®¡çš„æ–¹æ³•ï¼Œåˆ¤å®šç»¿è‰²çš„è¿™ä¸ªå¾…åˆ†ç±»ç‚¹å±äºè“è‰²çš„æ­£æ–¹å½¢ä¸€ç±»ã€‚

## äºŒã€ä¸‰è¦ç´ 

KNNç®—æ³•éœ€è¦è€ƒè™‘çš„ä¸‰ä¸ªé‡è¦å› ç´ ï¼škå€¼çš„é€‰å–ã€è·ç¦»åº¦é‡æ–¹å¼å’Œåˆ†ç±»å†³ç­–è§„åˆ™ã€‚

### 2.1 kå€¼çš„é€‰å–

kå€¼çš„é€‰æ‹©æ²¡æœ‰ä¸€ä¸ªå›ºå®šçš„ç»éªŒï¼Œä¸€èˆ¬å–ä¸€ä¸ªæ¯”è¾ƒå°çš„æ•°å€¼ã€‚é€šå¸¸é‡‡ç”¨**äº¤å‰éªŒè¯æ³•**æ¥é€‰æ‹©æœ€ä¼˜çš„kå€¼ã€‚

### 2.2 è·ç¦»åº¦é‡æ–¹å¼

ç‰¹å¾ç©ºé—´ä¸­ä¸¤ä¸ªå®ä¾‹ç‚¹çš„è·ç¦»æ˜¯ä¸¤ä¸ªå®ä¾‹ç‚¹**ç›¸ä¼¼ç¨‹åº¦**çš„åæ˜ ã€‚ç‰¹å¾ç©ºé—´ä¸€èˆ¬æ˜¯nç»´å®æ•°å‘é‡ç©ºé—´$\pmb{R^n}$ï¼ˆæ¬§å¼ç©ºé—´ï¼‰ã€‚ä¸€èˆ¬çš„$\pmb{L_p}$è·ç¦»$(L_p distance)$æˆ–é—µå¯å¤«æ–¯åŸº`(Minkowski)`è·ç¦»å®šä¹‰ï¼š

è®¾ç‰¹å¾ç©ºé—´ $\chi$ æ˜¯ $n$ ç»´å®æ•°å‘é‡ç©ºé—´ $\boldsymbol{R}^{n}, x_{i}, x_{j} \in \chi, x_{l}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \ldots, x_{i}^{(n)}\right)^{T}, \quad x_{j}=\left(x_{j}^{(1)}, x_{j}^{(2)}, \ldots, x_{j}^{(n)}\right)^{T}, x_{i}, x_{j}$ çš„ $L_{p}$ è·ç¦»å®šä¹‰ä¸ºï¼š
$$
L_{p}\left(x_{i}, x_{j}\right)=\left(\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{p}\right)^{\frac{1}{p}}
$$

+ è¿™é‡Œ$pâ‰¥1$ã€‚å½“$p=2$æ—¶ï¼Œç§°ä¸º**æ¬§æ°è·ç¦»**`ï¼ˆEuclidean distanceï¼‰`ï¼Œå³ï¼š

$$
L_{2}\left(x_{i}, x_{j}\right)=\left(\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{2}\right)^{\frac{1}{2}}
$$

+ å½“$p=1$æ—¶ï¼Œç§°ä¸º**æ›¼å“ˆé¡¿è·ç¦»**`ï¼ˆManhattan distanceï¼‰`ï¼Œå³ï¼š

$$
L_{1}\left(x_{i}, x_{j}\right)=\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|
$$

#### 2.2.1 æ¬§å¼è·ç¦»$(Euclidean\ distance)$

<div align=center>
<img src="/euclidean.jpg" alt="æ¬§æ°è·ç¦»" />
</div>
æœ€å¸¸è§çš„ä¸¤ç‚¹ä¹‹é—´æˆ–å¤šç‚¹ä¹‹é—´çš„è·ç¦»è¡¨ç¤ºæ–¹æ³•ï¼Œåˆç§°ä¸ºæ¬§å‡ é‡Œå¾—åº¦é‡ã€‚



å®šä¹‰äºæ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­ï¼Œç‚¹$x=(x1,â€¦,xn)$å’Œ$y=(y1,â€¦,yn)$ä¹‹é—´çš„è·ç¦»ä¸º:
$$
d(x, y)=\sqrt{\left(x_{1}-y_{1}\right)^{2}+\left(x_{2}-y_{2}\right)^{2}+\cdots+\left(x_{n}-y_{n}\right)^{2}}=\sqrt{\sum_{i=1}^{n}\left(x_{i}-y_{i}\right)^{2}}
$$

#### 2.2.2 æ›¼å“ˆé¡¿è·ç¦»($Manhattan\  distance$)

åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´çš„å›ºå®šç›´è§’åæ ‡ç³»ä¸Šä¸¤ç‚¹æ‰€å½¢æˆçš„çº¿æ®µå¯¹**è½´**äº§ç”Ÿçš„**æŠ•å½±**çš„è·ç¦»æ€»å’Œã€‚æ›¼å“ˆé¡¿è·ç¦»ä¹Ÿç§°ä¸ºâ€œåŸå¸‚è¡—åŒºè·ç¦»â€`(City Block distance)`ã€‚ä¾‹å¦‚åœ¨äºŒç»´å¹³é¢ä¸Šï¼Œåæ ‡$(x_1,y_1)$ä¸åæ ‡$(x_2,y_2)$çš„æ›¼å“ˆé¡¿è·ç¦»ä¸ºï¼š$|x_1âˆ’x_2|+|y_1âˆ’y_2|$ã€‚

(1) äºŒç»´å¹³é¢ä¸Šä¸¤ç‚¹$a(x_1,y_1)$ä¸$b(x_2,y_2)$é—´çš„æ›¼å“ˆé¡¿è·ç¦»ï¼š
$$
d_{12}=|x_1âˆ’x_2|+|y_1âˆ’y_2|
$$
(2) ä¸¤ä¸ª$n$ç»´å‘é‡$a(x_{11},x_{12},â€¦,x_{1n})$å’Œ$b(x_{21},x_{22},â€¦,x_{2n})$é—´çš„æ›¼å“ˆé¡¿è·ç¦»ï¼š
$$
d_{12}=\sum_{k=1}^{n}\left|x_{1 k}-x_{2 k}\right|
$$

<div align=center>
<img src="/manhattan.png" alt=æ›¼å“ˆé¡¿è·ç¦»" />
</div>

ä¸¤ç§è·ç¦»çš„**å¯¹æ¯”**ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºï¼šçº¢çº¿ä»£è¡¨æ›¼å“ˆé¡¿è·ç¦»ï¼Œç»¿è‰²ä»£è¡¨[æ¬§æ°è·ç¦»](https://baike.baidu.com/item/æ¬§æ°è·ç¦»)ï¼Œä¹Ÿå°±æ˜¯[ç›´çº¿è·ç¦»](https://baike.baidu.com/item/ç›´çº¿è·ç¦»)ï¼Œè€Œè“è‰²å’Œé»„è‰²åˆ†åˆ«ä»£è¡¨ç­‰ä»·çš„æ›¼å“ˆé¡¿è·ç¦»ã€‚

#### 2.2.3 åˆ‡æ¯”é›ªå¤«è·ç¦»($Chebyshev\ distance$)

å‘é‡ç©ºé—´ä¸­çš„ä¸€ç§åº¦é‡ï¼ŒäºŒä¸ªç‚¹ä¹‹é—´çš„è·ç¦»å®šä¹‰æ˜¯å…¶å„åæ ‡æ•°å€¼å·®ç»å¯¹å€¼çš„æœ€å¤§å€¼ã€‚è‹¥äºŒä¸ªå‘é‡æˆ–äºŒä¸ªç‚¹$pã€q$ï¼Œå…¶åæ ‡åˆ†åˆ«ä¸º$p_iã€q_i$ï¼Œåˆ™ä¸¤è€…ä¹‹é—´çš„åˆ‡æ¯”é›ªå¤«è·ç¦»å®šä¹‰å¦‚ä¸‹ï¼š
$$
D(p, q)=\max \left|p_{i}-q_{i}\right|
$$
(1) äºŒç»´å¹³é¢ä¸¤ç‚¹$a(x_1,y_1)$ä¸$b(x_2,y_2)$é—´çš„åˆ‡æ¯”é›ªå¤«è·ç¦»ï¼š
$$
d_{12}=max(|x_1âˆ’x_2|,|y_1âˆ’y_2|)
$$
(2) ä¸¤ä¸ª$n$ç»´å‘é‡$a(x_{11},x_{12},â€¦,x_{1n})$å’Œ$b(x_{21},x_{22},â€¦,x_{2n})$é—´çš„åˆ‡æ¯”é›ªå¤«è·ç¦»ï¼š
$$
d_{12}=\max _{i}\left(\left|x_{1 i}-x_{2 i}\right|\right)
$$
è¿™ä¸ªå…¬å¼çš„å¦ä¸€ç§ç­‰ä»·å½¢å¼æ˜¯ï¼š
$$
d_{12}=\lim _{k \rightarrow \infty}\left(\sum_{i=1}^{n}\left|x_{1 i}-x_{2 i}\right|^{k}\right)^{1 / k}
$$

<div align=center>
<img src="/chebyshev.jpg" alt="åˆ‡æ¯”é›ªå¤«è·ç¦»" />
</div>

å›½é™…è±¡æ£‹æ£‹ç›˜ä¸Šï¼Œå›½ç‹èµ°ä¸€æ­¥èƒ½å¤Ÿç§»åŠ¨åˆ°ç›¸é‚»çš„8ä¸ªæ–¹æ ¼ä¸­çš„ä»»æ„ä¸€ä¸ªï¼Œå…¶ä¸­åˆ‡æ¯”é›ªå¤«è·ç¦»æŒ‡ç‹è¦ä»ä¸€ä¸ªä½å­ç§»è‡³å¦ä¸€ä¸ªä½å­è‡³å°‘éœ€è¦èµ°çš„æ­¥æ•°ã€‚ä½ ä¼šå‘ç°ï¼Œå›½ç‹ä»æ ¼å­$(x_1,y_1)$èµ°åˆ°æ ¼å­$(x_2,y_2)$æœ€å°‘éœ€è¦çš„æ­¥æ•°æ€»æ˜¯$max(|x_2âˆ’x_1|,|y_2âˆ’y_1|)$æ­¥ ã€‚

#### 2.2.4 å¤¹è§’ä½™å¼¦è·ç¦»

å‡ ä½•ä¸­å¤¹è§’ä½™å¼¦å¯ç”¨æ¥è¡¡é‡ **ä¸¤ä¸ªå‘é‡æ–¹å‘çš„å·®å¼‚**ï¼Œåœ¨æœºå™¨å­¦ä¹ ä¸­ç‰¹å¾é€šå¸¸ä½¿ç”¨å‘é‡å½¢å¼æ¥è¡¨ç¤ºï¼Œå¸¸ç”¨è¿™ä¸€æ¦‚å¿µæ¥è¡¡é‡æ ·æœ¬å‘é‡ä¹‹é—´çš„å·®å¼‚ã€‚å‘é‡æ–¹å‘å·®å¼‚èŒƒå›´ï¼š$[âˆ’1,1]$

- äºŒç»´ç©ºé—´ä¸­çš„å‘é‡$(x_1,y_1)$)ä¸å‘é‡$(x_2,y_2)$å¤¹è§’ä½™å¼¦ï¼š

$$
\cos \theta=\frac{x_{1} x_{2}+y_{1} y_{2}}{\sqrt{x_{1}^{2}+y_{1}^{2}} \sqrt{x_{2}^{2}+y_{2}^{2}}}
$$

+ ç»™å®šä¸¤ä¸ªç‰¹å¾å‘é‡$A(x_{11},x_{12},â€¦,x_{1n})$å’Œ$B(x_{21},x_{22},â€¦,x_{2n})$ï¼Œå…¶ä½™å¼¦ç›¸ä¼¼æ€§$\theta$ç”±ç‚¹ç§¯å’Œå‘é‡é•¿åº¦ç»™å‡ºï¼Œå¦‚ä¸‹æ‰€ç¤º:

$$
\text { similarity }=\cos (\theta)=\frac{A \cdot B}{\|A\|\|B\|}=\frac{\sum_{i=1}^{n} A_{i} \times B_{i}}{\sqrt{\sum_{i=1}^{n}\left(A_{i}\right)^{2}} \times \sqrt{\sum_{i=1}^{n}\left(B_{i}\right)^{2}}}=\frac{\sum_{i=1}^{n} x_{1 i} x_{2 i}}{\sqrt{\sum_{i=1}^{n} x_{1 i}^{2}} \sqrt{\sum_{i=1}^{n} x_{2 i}^{2}}}
$$

**ä½™å¼¦è·ç¦»å’Œæ¬§æ°è·ç¦»çš„åŒºåˆ«**

<div align=center>
<img src="/euclidean.jpg" alt="æ¬§æ°è·ç¦»" style="zoom:80%;"/>
</div>

ä»ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œæ¬§æ°è·ç¦»è¡¡é‡çš„æ˜¯ç©ºé—´å„ç‚¹çš„ç»å¯¹è·ç¦»ï¼Œè·Ÿå„ä¸ªç‚¹æ‰€åœ¨çš„ä½ç½®åæ ‡ç›´æ¥ç›¸å…³ï¼›è€Œä½™å¼¦è·ç¦»è¡¡é‡çš„æ˜¯ç©ºé—´å‘é‡çš„å¤¹è§’ï¼Œæ›´åŠ ä½“ç°åœ¨æ–¹å‘ä¸Šçš„å·®å¼‚ã€‚ä½™å¼¦è·ç¦»å¸¸ç”¨äºå½¢å®¹ä¸¤ä¸ªç‰¹å¾å‘é‡ä¹‹é—´çš„å…³ç³»ï¼Œä¾‹å¦‚äººè„¸è¯†åˆ«ï¼Œæ¨èç³»ç»Ÿç­‰ã€‚

- å¯¹äºå‘é‡é‡$[0,1]$å’Œå‘é‡$[1,0]$è€Œè¨€ï¼ŒäºŒè€…çš„ä½™å¼¦è·å¾ˆå¤§ï¼Œè€Œæ¬§æ°è·ç¦»å¾ˆå°ï¼›
- å¯¹äºå‘é‡$[1,10]$å’Œå‘é‡$[10,100]$è€Œè¨€ï¼Œä½™å¼¦è·ç¦»ä¼šè®¤ä¸ºä¸¤ä¸ªç‰¹å¾å‘é‡è·ç¦»å¾ˆè¿‘ï¼›ä½†æ˜¾ç„¶è¿™ä¸¤ä¸ªç‰¹å¾å‘é‡æ˜¯æœ‰ç€æå¤§å·®å¼‚çš„ï¼Œæ­¤æ—¶æˆ‘ä»¬æ›´å…³æ³¨æ•°å€¼çš„ç»å¯¹å·®å¼‚ï¼Œåº”å½“ä½¿ç”¨æ¬§æ°è·ç¦»ã€‚

> **æ³¨ï¼šåœ¨CNNä¸­ï¼Œå¯¹ç‰¹å¾å‘é‡è¿›è¡ŒL2èŒƒæ•°å½’ä¸€åŒ–åï¼Œæ¬§å¼è·ç¦»ç­‰ä»·äºä½™å¼¦è·ç¦»ã€‚**

#### 2.2.5 æ±‰æ˜è·ç¦»

æ±‰æ˜è·ç¦»æ˜¯ä½¿ç”¨åœ¨æ•°æ®ä¼ è¾“å·®é”™æ§åˆ¶ç¼–ç é‡Œé¢çš„ï¼Œæ±‰æ˜è·ç¦»æ˜¯ä¸€ä¸ªæ¦‚å¿µï¼Œå®ƒè¡¨ç¤ºä¸¤ä¸ªï¼ˆç›¸åŒé•¿åº¦ï¼‰å­—å¯¹åº”ä½ä¸åŒçš„æ•°é‡ã€‚ä¹Ÿå³å°†å…¶ä¸­ä¸€ä¸ªå˜ä¸ºå¦å¤–ä¸€ä¸ªæ‰€éœ€è¦ä½œçš„æœ€å°æ›¿æ¢æ¬¡æ•°ã€‚ä¾‹å¦‚1011101 ä¸ 1001001 ä¹‹é—´çš„æ±‰æ˜è·ç¦»æ˜¯ 2ã€‚

### 2.3 ç‰¹å¾å½’ä¸€åŒ–

> æ•°æ®ä¸­ä¸åŒç‰¹å¾å€¼å·®è·ååˆ†å¤§ï¼Œå¯¼è‡´é¢„æµ‹ç»“æœè¢«æŸé¡¹ç‰¹å¾ä¸»å¯¼ï¼Œè€Œå¿½ç•¥äº†å…¶ä»–ç‰¹å¾çš„å½±å“ï¼Œæ‰€ä»¥éœ€è¦è¿›è¡Œæ•°æ®çš„å½’ä¸€åŒ–ã€‚
>
> è§£å†³æ–¹æ¡ˆï¼šå°†æ‰€æœ‰æ•°æ®æ˜ å°„åˆ°åŒä¸€å°ºåº¦ä¸Šã€‚

å¸¸ç”¨çš„ç‰¹å¾å½’ä¸€åŒ–æ–¹æ³•åŒ…æ‹¬**æœ€å€¼å½’ä¸€åŒ–**å’Œ**å‡å€¼æ–¹å·®å½’ä¸€åŒ–**ã€‚

#### 2.3.1 æœ€å€¼å½’ä¸€åŒ–($normalization$)

å°†æ‰€æœ‰æ•°æ®æ˜ å°„åˆ°0-1ä¹‹é—´ï¼š
$$
x_{\text {scale }}=\frac{x-x_{\min }}{x_{\max }-x_{\min }}
$$

```python
import numpy as np
import matplotlib.pyplot as plt
x = np.random.randint(0,100,100)
# äºŒç»´çŸ©é˜µä¸­åˆ†åˆ«å¯¹æ¯åˆ—è¿›è¡Œæœ€å€¼å½’ä¸€åŒ–
x = np.random.randint(0,100,(50,2))
x = np.array(x,dtype=float)
x[:,0] = (x[:,0] - np.min(x[:,0])) / (np.max(x[:,0]) - np.min(x[:,0]))
x[:,1] = (x[:,1] - np.min(x[:,1])) / (np.max(x[:,1]) - np.min(x[:,1]))
plt.scatter(x[:,0],x[:,1])
plt.show()
```

è¿è¡Œç»“æœï¼ˆå·¦å›¾ä¸ºå½’ä¸€åŒ–å‰ï¼Œå³å›¾ä¸ºå½’ä¸€åŒ–åï¼‰ï¼š

<div align=center>
    <img src="normalization_1.png" alt="æœ€å€¼å½’ä¸€åŒ–å‰" height="250"/>
    <img src="normalization_2.png" alt="æœ€å€¼å½’ä¸€åŒ–å" height="250"/>
</div>

#### 2.3.2 å‡å€¼æ–¹å·®å½’ä¸€åŒ–($standardization$)

å°†æ‰€æœ‰æ•°æ®å½’ä¸€åˆ°å‡å€¼ä¸º0æ–¹å·®ä¸º1çš„åˆ†å¸ƒä¸­ï¼š
$$
x_{\text {scale }}=\frac{x-x_{\text {mean }}}{s}
$$

```python
import numpy as np
import matplotlib.pyplot as plt
# äºŒç»´çŸ©é˜µä¸­åˆ†åˆ«å¯¹æ¯åˆ—è¿›è¡Œå‡å€¼æ–¹å·®å½’ä¸€åŒ–
x2 = np.random.randint(0,100,(50,2))
x2 = np.array(x2,dtype=float)
x2[:,0] = (x2[:,0] - np.mean(x2[:,0])) / np.std(x2[:,0])
x2[:,1] = (x2[:,1] - np.mean(x2[:,1])) / np.std(x2[:,1])
plt.scatter(x2[:,0],x2[:,1])
plt.show()
```

è¿è¡Œç»“æœï¼ˆå·¦å›¾ä¸ºå½’ä¸€åŒ–å‰ï¼Œå³å›¾ä¸ºå½’ä¸€åŒ–åï¼‰ï¼š

<div align=center>
    <img src="standardization_1.png" alt="å‡å€¼æ–¹å·®å½’ä¸€åŒ–å‰" height="250"/>
    <img src="standardization_2.png" alt="å‡å€¼æ–¹å·®å½’ä¸€åŒ–å" height="250"/>
</div>

#### 2.3.3 $scikit\_learn$ä¸­çš„å½’ä¸€åŒ–

<div align=center>
<img src="/scaler.png" alt="ä½¿ç”¨scalarè¿›è¡Œå½’ä¸€åŒ–" style="zoom:80%;"/>
</div>

```python
#scikit_learnä¸­çš„Scalar

import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split


#ä»¥é¸¢å°¾èŠ±çš„æ•°æ®é›†ä¸ºç¤ºä¾‹
iris = datasets.load_iris()
X=iris.data
y=iris.target

#åˆ›å»ºè®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†
X_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=666)

from sklearn.preprocessing import StandardScaler  #sklearnä¸­çš„ç›¸åº”çš„ç±»

standardScaler = StandardScaler() #  æ„é€ å‡å€¼æ–¹å·®å½’ä¸€åŒ–å¯¹è±¡
standardScaler.fit(X_train) # æ±‚å‡ºç›¸åº”çš„å‡å€¼å’Œæ–¹å·®ï¼ˆæ ¹æ®è®­ç»ƒé›†ï¼‰
standardScaler.mean_ # å‡å€¼ array([5.83416667, 3.0825    , 3.70916667, 1.16916667])
standardScaler.scale_  # æ ‡å‡†å·®array([0.81019502, 0.44076874, 1.76295187, 0.75429833])

X_train=standardScaler.transform(X_train)  # æ ¹æ®fitè®¡ç®—å‡ºæ¥çš„å€¼æ¥è¿›è¡Œç›¸åº”çš„æ•°æ®å½’ä¸€åŒ–

x_test_transform=standardScaler.transform(x_test)  # å¯¹æµ‹è¯•é›†ä¹Ÿä½¿ç”¨åŒæ ·çš„æ–¹æ³•è¿›è¡Œç›¸åº”çš„æ•°æ®å½’ä¸€åŒ–

from sklearn.neighbors import KNeighborsClassifier  
# åˆ›å»ºä¸€ä¸ªkNNåˆ†ç±»å™¨
knn_clf=KNeighborsClassifier(n_neighbors=3)
knn_clf.fit(X_train,y_train)

'''
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=3, p=2,
           weights='uniform')
'''
print(knn_clf.score(x_test,y_test)) # æ•°æ®å½’ä¸€åŒ–å‰çš„ç²¾ç¡®åº¦
print(knn_clf.score(x_test_transform,y_test)) # æ•°æ®å½’ä¸€åŒ–åçš„ç²¾ç¡®åº¦
```

> è¿è¡Œç»“æœï¼š
>
> ```python
> 0.3333333333333333
> 1.0
> ```



## ä¸‰ã€ç®—æ³•å®ç°

### 3.1 ç®€å•æ–¹æ³•

æŒ‰ç…§ké‚»è¿‘çš„æ€æƒ³ï¼Œæ‰¾åˆ°kkä¸ªæœ€è¿‘çš„é‚»å±…æ¥è¿›è¡Œé¢„æµ‹ï¼Œå°±éœ€è¦è®¡ç®—å‡º**é¢„æµ‹æ ·æœ¬**ä¸**æ‰€æœ‰è®­ç»ƒé›†**ä¸­æ ·æœ¬çš„è·ç¦»ï¼Œç„¶åè®¡ç®—å‡ºæœ€å°çš„kä¸ªè·ç¦»ï¼Œæ¥ç€è¿›è¡Œ**æŠ•ç¥¨è¡¨å†³**ï¼Œå³å¯åšå‡ºé¢„æµ‹ã€‚è¿™ç§æ€è·¯ç®€å•ç›´æ¥ï¼Œåœ¨**æ ·æœ¬é‡å°‘ã€ç‰¹å¾å°‘**çš„æƒ…å†µä¸‹æœ‰æ•ˆã€‚å¯¹äºå¤§é‡çš„æ•°æ®è€Œè¨€ï¼Œ**ç‰¹è¯æ•°**å’Œ**æ ·æœ¬é‡**éƒ½å¾ˆå¤§ï¼Œå¦‚æœè¦é¢„æµ‹**å°‘é‡**çš„æµ‹è¯•é›†æ ·æœ¬ï¼Œç®—æ³•çš„æ—¶é—´æ•ˆç‡ä¼šå¾ˆä½ã€‚

### 3.2 KDæ ‘

kdæ ‘`(K-dimension tree)`æ˜¯ä¸€ç§å¯¹kç»´ç©ºé—´ä¸­çš„å®ä¾‹ç‚¹è¿›è¡Œå­˜å‚¨ä»¥ä¾¿å¯¹å…¶è¿›è¡Œå¿«é€Ÿæ£€ç´¢çš„æ ‘å½¢æ•°æ®ç»“æ„ã€‚kdæ ‘æ˜¯ä¸€ç§**äºŒå‰æ ‘**ï¼Œè¡¨ç¤ºå¯¹kç»´ç©ºé—´çš„ä¸€ä¸ª**åˆ’åˆ†**ï¼Œæ„é€ kdæ ‘ç›¸å½“äºä¸æ–­åœ°ç”¨å‚ç›´äºåæ ‡è½´çš„è¶…å¹³é¢å°†**kç»´ç©ºé—´åˆ‡åˆ†**ï¼Œæ„æˆä¸€ç³»åˆ—çš„kç»´è¶…çŸ©å½¢åŒºåŸŸã€‚åˆ©ç”¨kdæ ‘å¯ä»¥çœå»å¯¹å¤§éƒ¨åˆ†æ•°æ®ç‚¹çš„æœç´¢ï¼Œä»è€Œå‡å°‘æœç´¢çš„è®¡ç®—é‡ã€‚

<div align=center>
<img src="/kd_function.jpg" alt="KDæ–¹æ³•è¾“å…¥è¾“å‡º"/>
</div>

### 3.3 æ„é€ å¹³è¡¡kdæ ‘ç®—æ³•

è¾“å…¥ï¼š**kç»´**ç©ºé—´æ•°æ®é›†$T=x_1,x_2,â€¦,x_N$ï¼Œå…¶ä¸­$x_i=(x_i^{(1)},x_i^{(2)},â€¦,x_i^{(k)}),i=1,2,â€¦N$

- å¼€å§‹ï¼šæ„é€ æ ¹èŠ‚ç‚¹ï¼Œé€‰æ‹©$x^{(1)}$ä¸ºåæ ‡è½´ï¼Œä»¥$T$ä¸­æ‰€æœ‰å®ä¾‹çš„$x^(1)$åæ ‡çš„**ä¸­ä½æ•°**ä¸ºåˆ‡ç‚¹ï¼Œå°†æ ¹ç»“ç‚¹å¯¹åº”çš„è¶…çŸ©å½¢åŒºåŸŸåˆ‡åˆ†ä¸ºä¸¤ä¸ª**å­åŒºåŸŸ**ã€‚ç”±æ ¹èŠ‚ç”Ÿæˆæ·±åº¦$1$çš„å·¦å³å­èŠ‚ç‚¹ï¼Œå·¦ã€å³å­ç»“ç‚¹åˆ†åˆ«å¯¹åº”åæ ‡$x^{(1)}$å°äºã€å¤§äºåˆ‡åˆ†ç‚¹çš„å­åŒºåŸŸã€‚**å°†è½åœ¨åˆ‡åˆ†å¹³é¢ä¸Šçš„å®ä¾‹ç‚¹ä¿å­˜åœ¨è¯¥ç»“ç‚¹ã€‚**
- é‡å¤ï¼šå¯¹æ·±åº¦ä¸º$\pmb j$çš„ç»“ç‚¹ï¼Œé€‰æ‹©x(l)x(l)ä¸ºåˆ‡åˆ†çš„åæ ‡è½´ï¼Œ$\pmb {l=j\ \% \ k+1}$ï¼Œä»¥è¯¥ç»“ç‚¹çš„åŒºåŸŸä¸­æ‰€æœ‰å®ä¾‹çš„$x^{(l)}$åæ ‡çš„ä¸­ä½æ•°ä¸ºåˆ‡åˆ†ç‚¹ï¼Œå°†è¯¥ç»“ç‚¹å¯¹åº”çš„è¶…çŸ©å½¢åŒºåŸŸåˆ‡åˆ†ä¸ºå·¦å³ä¸¤ä¸ªå­åŒºåŸŸã€‚**å°†è½åœ¨åˆ‡åˆ†å¹³é¢ä¸Šçš„å®ä¾‹ç‚¹ä¿å­˜åœ¨è¯¥ç»“ç‚¹ã€‚**

**ç®€å•çš„äºŒç»´å¹³é¢çš„ä¾‹å­ï¼š**

ç»™å®šä¸€ä¸ªäºŒç»´æ•°æ®é›†ï¼š$T={(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)}$ï¼Œæ„é€ ä¸€ä¸ªå¹³è¡¡kdæ ‘ã€‚

1. å¼€å§‹ï¼šé€‰æ‹©$x^{(1)}$è½´ï¼Œ $6$ä¸ªæ•°æ®çš„å$x^{(1)}$æ ‡**ä¸­ä½æ•°**æ˜¯$6$ï¼Œè¿™é‡Œé€‰**æœ€æ¥è¿‘çš„**$(7,2)$ç‚¹ï¼Œä»¥å¹³é¢å°†$x^{(1)}=7$ç©ºé—´åˆ†ä¸ºå·¦å³ä¸¤ä¸ªå­çŸ©å½¢ï¼Œå¹¶å°†$(7,2)$ç‚¹ä¿å­˜åœ¨æ ¹èŠ‚ç‚¹ï¼›
2. é‡å¤ï¼šæ¥ç€è®¡ç®—å·¦çŸ©å½¢ä¸­$(2,3),(5,4),(4,7)$ç‚¹çš„$x^{(2)}$å æ ‡ä¸­ä½æ•°ä¸º$4$ï¼Œå·¦çŸ©é˜µä»¥$x^{(2)}=4$åˆ†ä¸ºä¸¤ä¸ªå­çŸ©å½¢ï¼Œå¹¶å°†$(5,4)$ç‚¹ä¿å­˜åœ¨å·¦å­èŠ‚ç‚¹ï¼›å†è®¡ç®—å³çŸ©å½¢ä¸­$(8,1),(9,6)$ç‚¹çš„$x^{(2)}$åæ ‡ä¸­ä½æ•°ä¸º$6$ï¼Œå·¦çŸ©é˜µä»¥$x^{(2)}=6$åˆ†ä¸ºä¸¤ä¸ªå­çŸ©å½¢ï¼Œå¹¶å°†$(9,6)$ç‚¹ä¿å­˜åœ¨å³å­èŠ‚ç‚¹ï¼›å¦‚æ­¤**é€’å½’**ï¼Œæœ€åå¾—åˆ°å¦‚ä¸‹å›¾æ‰€ç¤ºçš„å¹³è¡¡$kd$æ ‘ã€‚

<div align=center>
<img src="/2DExample.png" alt="äºŒç»´å¹³é¢æ„é€ KDæ ‘" style="zoom:80%;"/>
</div>

### 3.4 æœç´¢kdæ ‘

åˆ©ç”¨kdæ ‘å¯ä»¥çœå»å¯¹å¤§éƒ¨åˆ†æ•°æ®ç‚¹çš„æœç´¢ï¼Œä»è€Œå‡å°‘æœç´¢çš„è®¡ç®—é‡ã€‚ç»™å®šä¸€ä¸ªç›®æ ‡ç‚¹ï¼Œæœç´¢å…¶æœ€è¿‘é‚»ï¼Œé¦–å…ˆæ‰¾åˆ°åŒ…å«ç›®æ ‡ç‚¹çš„**å¶èŠ‚ç‚¹**ï¼›ç„¶åä»è¯¥å¶ç»“ç‚¹å‡ºå‘ï¼Œä¾æ¬¡**å›é€€**åˆ°**çˆ¶ç»“ç‚¹ï¼›**ä¸æ–­æŸ¥æ‰¾ä¸ç›®æ ‡ç‚¹æœ€è¿‘é‚»çš„ç»“ç‚¹ï¼Œå½“ç¡®å®š**ä¸å¯èƒ½å­˜åœ¨æ›´è¿‘**çš„ç»“ç‚¹æ—¶ç»ˆæ­¢ã€‚

> è¾“å…¥ï¼šå·²æ„é€ çš„kdæ ‘ï¼Œç›®æ ‡ç‚¹$x$;

1. åœ¨kdæ ‘ä¸­æ‰¾å‡ºåŒ…å«ç›®æ ‡ç‚¹xçš„å¶ç»“ç‚¹ï¼šä»æ ¹ç»“ç‚¹å‡ºå‘ï¼Œé€’å½’çš„å‘ä¸‹è®¿é—®$kd$æ ‘ã€‚è‹¥ç›®æ ‡ç‚¹å½“å‰ç»´çš„åæ ‡å€¼**å°äº**åˆ‡åˆ†ç‚¹çš„åæ ‡å€¼ï¼Œåˆ™ç§»åŠ¨åˆ°å·¦å­ç»“ç‚¹ï¼Œå¦åˆ™ç§»åŠ¨åˆ°å³å­ç»“ç‚¹ã€‚ç›´åˆ°å­ç»“ç‚¹ä¸º**å¶ç»“ç‚¹**ä¸ºæ­¢ï¼›
2. ä»¥æ­¤å¶ç»“ç‚¹ä¸º**å½“å‰æœ€è¿‘ç‚¹**ï¼›
3. é€’å½’å‘ä¸Šå›é€€ï¼Œå¦‚æœ**è¯¥ç»“ç‚¹ä¿å­˜çš„å®ä¾‹ç‚¹**æ¯”**å½“å‰æœ€è¿‘ç‚¹**è·ç›®æ ‡ç‚¹**æ›´è¿‘**ï¼Œåˆ™ä»¥è¯¥å®ä¾‹ç‚¹ä¸ºå½“å‰æœ€è¿‘ç‚¹ï¼›è‹¥å½“å‰æœ€è¿‘ç‚¹ä¸€å®šå­˜åœ¨äºè¯¥ç»“ç‚¹ä¸€ä¸ªå­ç»“ç‚¹å¯¹åº”çš„åŒºåŸŸã€‚æ£€æŸ¥è¯¥å­ç»“ç‚¹çš„çˆ¶ç»“ç‚¹çš„**å¦ä¸€ä¸ªå­ç»“ç‚¹**å¯¹åº”çš„åŒºåŸŸæ˜¯å¦æœ‰æ›´è¿‘çš„ç‚¹ã€‚
4. å›é€€åˆ°æ ¹èŠ‚ç‚¹ï¼Œæœç´¢ç»“æŸã€‚æœ€åçš„**å½“å‰æœ€è¿‘ç‚¹**å³ä¸ºxçš„æœ€è¿‘é‚»ç‚¹ã€‚

**ä»¥å…ˆå‰æ„å»ºå¥½çš„kdæ ‘ä¸ºä¾‹ï¼ŒæŸ¥æ‰¾ç›®æ ‡ç‚¹$(3,4.5)$çš„æœ€è¿‘é‚»ç‚¹ï¼š**

1. é¦–å…ˆï¼Œé€šè¿‡**æœç´¢è·¯å¾„**$(7,2)â†’(5,4)â†’(4,7)$ï¼Œæ‰¾åˆ°**æ ¹èŠ‚ç‚¹**$(4,7)$å–$(4,7)$ä¸ºå½“å‰æœ€è¿‘èŠ‚ç‚¹ã€‚
2. å–$(4,7)$ä¸ºå½“å‰æœ€è¿‘é‚»ç‚¹ã€‚ä»¥ç›®æ ‡æŸ¥æ‰¾ç‚¹ä¸ºåœ†å¿ƒï¼Œç›®æ ‡æŸ¥æ‰¾ç‚¹åˆ°å½“å‰æœ€è¿‘ç‚¹çš„è·ç¦»$2.69$ä¸ºåŠå¾„ç¡®å®šä¸€ä¸ªçº¢è‰²çš„åœ†ã€‚ç„¶åå›æº¯åˆ°$(5,4)$ï¼Œè®¡ç®—å…¶ä¸æŸ¥æ‰¾ç‚¹ä¹‹é—´çš„è·ç¦»ä¸º$2.06$ï¼Œåˆ™è¯¥ç»“ç‚¹æ¯”å½“å‰æœ€è¿‘ç‚¹è·ç›®æ ‡ç‚¹æ›´è¿‘ï¼Œä»¥$(5,4)$ä¸ºå½“å‰æœ€è¿‘ç‚¹ã€‚
3. åŒæ ·çš„æ–¹æ³•ç¡®å®šä¸€ä¸ªç»¿è‰²çš„åœ†ï¼Œè¯¥åœ†$y=4$å¹³é¢ç›¸äº¤ï¼Œè¿›å…¥$(5,4)$ç»“ç‚¹çš„å¦ä¸€ä¸ªå­ç©ºé—´è¿›è¡ŒæŸ¥æ‰¾ã€‚ç»“ç‚¹$(2,3)$ä¸ç›®æ ‡ç‚¹è·ç¦»ä¸º$1.8$ï¼Œæ¯”å½“å‰æœ€è¿‘ç‚¹è¦è¿›ï¼Œæœ€è¿‘é‚»ç‚¹æ›´æ–°ä¸º$(2,3)$ã€‚
4. æ ¹æ®è§„åˆ™ç¡®å®šè“è‰²çš„åœ†ï¼Œè¯¥åœ†**ä¸$x=7$å¹³é¢ä¸ç›¸äº¤**ï¼Œä¸ç”¨å†è¿›å…¥å­ç©ºé—´è¿›è¡ŒæŸ¥æ‰¾ã€‚
5. è‡³æ­¤ï¼Œå›æº¯å®Œæ¯•ï¼Œè¿”å›**æœ€è¿‘é‚»ç‚¹**$(2,3)$ã€‚

<div align=center>
    <img src="search.jpg" alt="æœç´¢è¿‡ç¨‹" height="300"/>
    <img src="result.png" alt="KDæ ‘æ„é€ ç»“æœ" height="300"/>
</div>

**ä»£ç å®ç°ï¼ˆcs231nï¼‰**

<div align=center>
<img src="/cifar-10.png" alt="cs231nè¯¾ç¨‹ä¸­ä½¿ç”¨Kè¿‘é‚»ç®—æ³•å¯¹cifar-10æ•°æ®åˆ†ç±»"/>
</div>

```python
import numpy as np

class KNearestNeighbor(object):
    """"a kNN classifiers with L2 distance"""

    def __init__(self):
        pass

    def train(self,X,y):
        """
        Train the classifier. For k-nearest neighbors this is just
        memorizing the training data.
        Inputs:
        - X: A numpy array of shape (num_train, D) containing the training data
          consisting of num_train samples each of dimension D.
        - y: A numpy array of shape (N,) containing the training labels, where
             y[i] is the label for X[i].
        """
        self.X_train = X
        self.y_train = y

    def predict(self,X,k=1,num_loops=0):
        """
        :param X: A numpy array of shape (num_test, D) containing test data consisting
             of num_test samples each of dimension D.
        :param k: The number of nearest neighbors that vote for the predicted labels.
        :param num_loops: Determines which implementation to use to compute distances
          between training points and testing points.
        :return: A numpy array of shape (num_test,) containing predicted labels for the
          test data, where y[i] is the predicted label for the test point X[i].
        """

        if num_loops == 0:
            dists = self.compute_distances_no_loops(X)
        elif num_loops == 1:
            dists = self.compute_distances_one_loop(X)
        elif num_loops == 2:
            dists = self.compute_distances_two_loops(X)
        else:
            raise ValueError('Invalid value %d for num_loops' % num_loops)

        return self.predict_labels(dists,k=k)

    def compute_distances_two_loops(self, X):
        """
        Compute the distance between each test point in X and each training point
        in self.X_train using a nested loop over both the training data and the
        test data.
        Inputs:
        - X: A numpy array of shape (num_test, D) containing test data.
        Returns:
        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]
          is the Euclidean distance between the ith test point and the jth training
          point.
        """
        num_test = X.shape[0] # æµ‹è¯•æ•°æ®è¡Œæ•°
        num_train = self.X_train.shape[0] # è®­ç»ƒæ•°æ®å‡½è¡Œæ•°
        dists = np.zeros((num_test,num_train))
        for i in range(num_test):
            for j in range(num_train):
                #####################################################################
                # TODO:                                                             #
                # Compute the l2 distance between the ith test point and the jth    #
                # training point, and store the result in dists[i, j]. You should   #
                # not use a loop over dimension.                                    #
                #####################################################################
                num_test_i = X[i,:]  # num_testçš„ç¬¬iè¡Œ
                num_train_i = self.X_train[j,:]  # num_trainçš„ç¬¬jè¡Œ
                 # æ±‚å¾—æµ‹è¯•æ•°æ®çš„ç¬¬iè¡Œè·Ÿç¬¬è®­ç»ƒæ•°æ®çš„ç¬¬jè¡Œå·®å€¼çš„å¹³æ–¹å’Œ
                num_sum = np.sum((num_test_i - num_train_i) ** 2)
                dists[i,j] = np.sqrt(num_sum)  # å¼€å¹³æ–¹æ ¹æ±‚è·ç¦»
                #####################################################################
                #                       END OF YOUR CODE                            #
                #####################################################################
        return dists

    def compute_distances_one_loop(self, X):
        """
        Compute the distance between each test point in X and each training point
        in self.X_train using a single loop over the test data.
        Input / Output: Same as compute_distances_two_loops
        """

        num_test = X.shape[0]
        num_train = self.X_train.shape[0]
        dists = np.zeros((num_test,num_train))
        for i in range(num_test):
            #######################################################################
            # TODO:                                                               #
            # Compute the l2 distance between the ith test point and all training #
            # points, and store the result in dists[i, :].                        #
            #######################################################################
            num_test_i = X[i,:]  # æµ‹è¯•æ•°æ®çš„ç¬¬iè¡Œ
            num_row = np.sum((num_test_i - self.X_train) ** 2,axis=1)  # è¿”å›æ¯è¡Œçš„è®¡ç®—å¹³æ–¹å’Œçš„ç»“æœ
            dists[i,:] = np.sqrt(num_row)  # ç›´æ¥èµ‹å€¼ç¬¬iè¡Œ
            #######################################################################
            #                         END OF YOUR CODE                            #
            #######################################################################
        return dists

    def compute_distances_no_loops(self, X):
        """
        Compute the distance between each test point in X and each training point
        in self.X_train using no explicit loops.
        Input / Output: Same as compute_distances_two_loops
        """
        num_test = X.shape[0]  # ç¬¬ä¸€ç»´çš„å¤§å°
        num_train = self.X_train.shape[0]  # è®­ç»ƒæ•°æ®çš„ç¬¬ä¸€ç»´å¤§å°
        dists = np.zeros((num_test,num_train))
        #########################################################################
        # TODO:                                                                 #
        # Compute the l2 distance between all test points and all training      #
        # points without using any explicit loops, and store the result in      #
        # dists.                                                                #
        #                                                                       #
        # You should implement this function using only basic array operations; #
        # in particular you should not use functions from scipy.                #
        #                                                                       #
        # HINT: Try to formulate the l2 distance using matrix multiplication    #
        #       and two broadcast sums.                                         #
        #########################################################################
        X_sum = np.sum(np.power(X,2),axis=1)  # æ¯ä¸€è¡Œå¹³æ–¹å’Œ
        X_C_1 = np.reshape(X_sum,(-1,1))  # è¡Œå‘é‡è½¬ä¸ºåˆ—å‘é‡
        X_C_2 = -2 * np.dot(X,self.X_train.T)  # -2å€çš„AÂ·ğµğ‘‡
        X_C_3 = np.sum(np.power(self.X_train,2),axis=1)
        dists = np.sqrt(X_C_1 + X_C_2 + X_C_3)
        #########################################################################
        #                         END OF YOUR CODE                              #
        #########################################################################
        return dists

    def predict_labels(self, dists, k=1):
        """
        Given a matrix of distances between test points and training points,
        predict a label for each test point.
        Inputs:
        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]
          gives the distance betwen the ith test point and the jth training point.
        Returns:
        - y: A numpy array of shape (num_test,) containing predicted labels for the
          test data, where y[i] is the predicted label for the test point X[i].
        """
        num_test = dists.shape[0]
        y_pred = np.zeros(num_test)
        for i in range(num_test):
            # A list of length k storing the labels of the k nearest neighbors to
            # the ith test point.
            closest_y = []
            #########################################################################
            # TODO:                                                                 #
            # Use the distance matrix to find the k nearest neighbors of the ith    #
            # testing point, and use self.y_train to find the labels of these       #
            # neighbors. Store these labels in closest_y.                           #
            # Hint: Look up the function numpy.argsort.                             #
            #########################################################################
            nearrest = np.argsort(dists[i,:])  # è¿”å›æ¯ä¸€è¡Œæ’åºåçš„ç»“æœç´¢å¼•
            closest_y = [self.y_train[i] for i in nearrest[:k]]  # è·ç¦»æœ€è¿‘çš„å‰kä¸ªå…ƒç´ çš„y_trainå€¼
            #########################################################################
            # TODO:                                                                 #
            # Now that you have found the labels of the k nearest neighbors, you    #
            # need to find the most common label in the list closest_y of labels.   #
            # Store this label in y_pred[i]. Break ties by choosing the smaller     #
            # label.                                                                #
            #########################################################################
            dict = {}
            for key in closest_y:  # éå†è®¡ç®—æ¯ä¸ªkeyå‡ºç°çš„æ¬¡æ•°
                if key in dict:
                    dict[key] += 1
                else:
                    dict[key] = 1
            # for key in closest_y:  # éå†è®¡ç®—æ¯ä¸ªkeyå‡ºç°çš„æ¬¡æ•°
            #     dict[key] = dict.get(key,0) + 1
            common_label,common_value = max(dict.items(),key=lambda item:item[1])  # æ‰¾å‡ºå‡ºç°æ¬¡æ•°æœ€å¤šçš„label
            y_pred[i] = common_label
            #########################################################################
            #                           END OF YOUR CODE                            #
            #########################################################################
        return y_pred
```

## å››ã€KNNç®—æ³•çš„ç¼ºé™·

<div align=center>
<img src="/knn_bug.jpg" alt="knnç®—æ³•çš„ç¼ºé™·"/>
</div>

- è§‚å¯Ÿä¸Šå›¾ï¼Œå¯ä»¥çœ‹åˆ°å¯¹äºæ ·æœ¬$X_u$ï¼Œé€šè¿‡knnç®—æ³•ï¼Œæ˜¾ç„¶å¯ä»¥å¾—åˆ°$X_u$åº”å±äº$w_1$ï¼Œä½†å¯¹äºæ ·æœ¬$Y$ï¼Œé€šè¿‡ knnç®—æ³•æœ€ç»ˆä¼¼ä¹å¾—åˆ°äº†$Y$åº”å±äº$w_2$çš„ç»“è®ºï¼Œè€Œè¿™ä¸ªç»“è®ºç›´è§‚æ¥çœ‹å¹¶æ²¡æœ‰è¯´æœåŠ›ã€‚
- å½“æ ·æœ¬**ä¸å¹³è¡¡**æ—¶ï¼Œknnä¼¼ä¹åªå…³å¿ƒå“ªç±»æ ·æœ¬çš„æ•°é‡æœ€å¤šï¼Œè€Œä¸å»æŠŠè·ç¦»è¿œè¿‘è€ƒè™‘åœ¨å†…
- æ”¹è¿›**ï¼šå¯ä»¥é‡‡ç”¨**æƒå€¼**çš„æ–¹æ³•æ¥æ”¹è¿›ã€‚å’Œè¯¥æ ·æœ¬è·ç¦»å°çš„é‚»å±…æƒå€¼å¤§ï¼Œå’Œè¯¥æ ·æœ¬è·ç¦»å¤§çš„é‚»å±…æƒå€¼åˆ™ç›¸å¯¹è¾ƒå°ï¼Œä»¥æ­¤æ¥é¿å…å› ä¸€ä¸ªæ ·æœ¬è¿‡å¤§å¯¼è‡´**è¯¯åˆ¤**çš„æƒ…å†µã€‚

## Reference

- [ã€Šæœºå™¨å­¦ä¹ ã€‹ å‘¨å¿—åè‘—](https://book.douban.com/subject/26708119/)
- [cs2331n](http://cs231n.stanford.edu/)